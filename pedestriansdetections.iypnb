# Pedestrian Detection in Jupyter Notebook

## Imports
import cv2
from ultralytics import YOLO
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import supervision as sv
#cv2: OpenCV for image/video processing.

#YOLO (from ultralytics): For real-time object detection.

#matplotlib, pandas, numpy, tqdm: For plotting, data handling, math operations, and progress bars.

#supervision: For video frame access and annotation.

#tkinter: GUI creation for file selection and user interaction.


# Display images inline in notebook
%matplotlib inline

## Utility Functions
#Resize Frame
def resize_frame(frame, scale_percent):
    width = int(frame.shape[1] * scale_percent / 100)
    height = int(frame.shape[0] * scale_percent / 100)
    dim = (width, height)
    return cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)

    
#Keep only the latest 'patience' number of tracking points per object to reduce memory usage and clutter.
def filter_tracks(centers, patience):
    return {k: dict(list(i.items())[-patience:]) for k, i in centers.items()}

#Maintains object IDs.
#Checks if the new detected object is near an old one â†’ updates that ID.
#If it's far, assigns a new ID.
def update_tracking(centers_old, obj_center, thr_centers, lastKey, frame, frame_max):
    is_new = 0
    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) 
               for k, center in centers_old.items()]
    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]
    previous_pos = [(k, obj_center) for k, centers in lastpos 
                    if np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers]
    if previous_pos:
        id_obj = previous_pos[0][0]
        centers_old[id_obj][frame] = obj_center
    else:
        if lastKey:
            last = lastKey.split('D')[1]
            id_obj = 'ID' + str(int(last)+1)
        else:
            id_obj = 'ID0'
        is_new = 1
        centers_old[id_obj] = {frame: obj_center}
        lastKey = list(centers_old.keys())[-1]
    return centers_old, id_obj, is_new, lastKey

## Main Detection Function

def detect_pedestrians(video_path, regions, scale_percent=100):
    model = YOLO('yolov8x.pt')
    video = cv2.VideoCapture(video_path)

    conf_level = 0.25
    class_IDS = [0]
    thr_centers = 30
    frame_max = 10
    patience = 100
    alpha = 0.3

    centers_old = {}
    lastKey = ''

    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
    fps = video.get(cv2.CAP_PROP_FPS)

    if scale_percent != 100:
        width = int(width * scale_percent / 100)
        height = int(height * scale_percent / 100)

    #Region of Interest(roi)
    #Lets the user draw polygons (4 points) on the first frame of the video for each ROI.
    #Converts selected points into rectangular slices for tracking.
    #Returns all defined ROIs.

    rois = extract_roi_from_video(video_path, regions)
    roi_counts = {roi['name']: 0 for roi in rois}

    for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):
        isFrame, frame = video.read()
        if not isFrame:
            break

        for roi in rois:
            x_range, y_range = roi['range']
            ROI = frame[y_range[0]:y_range[1], x_range[0]:x_range[1]]

            y_hat = model.predict(ROI, conf=conf_level, classes=class_IDS, device='cpu', verbose=False)

            boxes = y_hat[0].boxes.xyxy.cpu().numpy()
            conf = y_hat[0].boxes.conf.cpu().numpy()
            classes = y_hat[0].boxes.cls.cpu().numpy()

            for ix, box in enumerate(boxes):
                xmin, ymin, xmax, ymax = box.astype(int)
                center_x, center_y = int((xmax + xmin)/2), int((ymax + ymin)/2)

                centers_old, id_obj, is_new, lastKey = update_tracking(
                    centers_old, (center_x, center_y), thr_centers, lastKey, i, frame_max)
                roi_counts[roi['name']] += is_new

                # Optional visualization code here

            centers_old = filter_tracks(centers_old, patience)

    video.release()
    return roi_counts

## ROI Extraction (in Notebook)

def extract_roi_from_video(video_path, regions):
    video_info = sv.VideoInfo.from_video_path(video_path)
    generator = sv.get_video_frames_generator(video_path)
    frame = next(iter(generator))

    rois = []
    for region_name in regions:
        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        plt.title(f"Select 4 points for {region_name}")
        points = plt.ginput(4)
        plt.close()

        x_range = [max(min(p[0] for p in points), 0), min(max(p[0] for p in points), video_info.width - 1)]
        y_range = [max(min(p[1] for p in points), 0), min(max(p[1] for p in points), video_info.height - 1)]

        rois.append({
            "name": region_name,
            "polygon": points,
            "range": [list(map(int, x_range)), list(map(int, y_range))]
        })
    return rois
